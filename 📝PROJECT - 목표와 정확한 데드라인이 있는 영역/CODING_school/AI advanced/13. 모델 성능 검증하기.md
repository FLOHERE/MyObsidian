- 예측 결과의 신뢰도를 높이는 방법을 알아봄
## 1. 데이터의 확인과 예측 실행
```python
df = pd.read_csv('./data.sonar3.csv', header = None)
# header : csv 파일의 제일 윗부분, 어떤 데이터인지 정보가 적혀있는 부분을 None으로 설정하면 없애고, True로 설정하면 보여줌
```

- 코드 실행 결과
	- 결과 데이터(소수점) : 0~1까지 평준화 (한번 가공한 데이터)
	- 맨 마지막 60번째 줄 : Label(실제 데이터)
		- 0 또는 1로 나온다. (binary classification)

![[Pasted image 20250621201038.png]]

- 실행 결과 분석
	- 전체가 61개의 열로 되어 있다.
	- 일반 암석 : 0
	- 광석 : 1
	- 1~60번째 열 : 음파 주파수의 에너지(0또는 1로 나타냄)

- 일반 암속과 광석이 각각 몇개 포함되는지 알아보는 코드
```python
df[60].value_counts() # 60번째 열의 데이터 수를 불러온다.
```

- 실행 결과
	- 1 : 광석의 개수 (111개)
	- 0 : 일반석의 개수 (97개)
	- `int 64` : 8바이트로 이루어진 데이터다.
	- 총 샘플 수 : 광석 개수 + 일반석 개수(208)
```txt
1    111 
0    97
Name : 60, dtype : int 64
```

```python
X = df.iloc[:, 0:60] # 0~59 번째 열 까지의 데이터 값을 가지고 오겠다는 뜻
y = df.iloc[:, 60] # 0 or 1 의 값으로 출력하겠다는 뜻
```

### ==시험문제 낼거다.== 초음파 광물 예측하기 : 데이터 확인과 실행

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 모델을 설정 합니다.
model = Sequential()
model.add(Dense(24, input_dim = 60, activation = 'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

# 모델 컴파일 합니다.
model.compile(loss = 'binary_crossentropy', optimizer = 'adam')
metrics = ['accuracy'] # loss + accuracy 값도 같이 나옴

# 모델을 실행 합니다.
history = model.fit(X, y, epochs = 200, batch_size = 10)
```

![[Pasted image 20250621204027.png]]

- 200번 반복되었을때 정확도가 100%가 나온다.
	- 이는 트레이닝 데이터(학습 데이터)를 그대로 테스트 데이터로 사용했기 때문에 올바른 정확도가 아니다. 
	- 테스트 데이터로 정확도를 다시 측정 해야 한다.
- **Binary Crossentropy**:

$$
\text{Loss} = - \left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]
$$
- y : 실제값 (0 또는 1)
- ŷ  : 예측값 (0 ~ 1 사이의 확률값)

>[!note] X 는 대문자고 y는 소문자인데 무슨 차이 인가여;;
> X 는 값이 두개 이상 나오는 "vector" 이고, y는 출력층 이므로 sigmoid 함수를 사용하기 때문에 값이 하나가 나오므로, 둘을 구분 하기 위해 각자 대문자, 소문자로 씁니다.

## 2. 과적합 이해하기
- 과적합(overfiting) : 학습 데이터에서는 일정 수준 이상의 예측 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는것
	- 학습셋 내부 성공률은 높아지나, 테스트셋에서 효과가 없다면 과적합이 일어나고 있는것.

![[Pasted image 20250621211734.png]]

- 과적합이 일어나는 이유
	1. 층이 너무 많을때
	2. 변수가 복잡할때
	3. 테스트 셋과 학습 셋이 중복 될때

## 3. 학습과 테스트 셋
- 과적합을 방지 하려면?
	- 학습셋과 테스트셋의 구분으로 해결
	- 학습과 동시에 테스트를 병행하며 진행하는 방법이 효율적
		- ex_) 총 데이터셋이 100개라면 70개는 학습셋으로, 나머지 30개는 테스트 셋으로 설정
- 모델 : 학습 결과를 저장한 파일
	- 목표 : w, b 값 구하기
	- ex_) 신경망을 만들어 70개의 샘플로 학습 진행 -> 학습 결과 저장
	- 다른셋에 적용하여 그대로 다시 수행(재활용) 가능(다시 학습 시킬 필요 X)
	- 30개의 테스트셋으로 실험해 정확도를 살펴볼수 있다.(학습이 얼마나 잘되었는지 확인 가능)
- ==학습과 테스트 셋 순서==
	1. 학습셋 삽입
	2. 딥러닝 학습
	3. 모델에서 가중치와 바이어스 구함
		- `.hdf5` 파일 확장자로 저장 -> FPU, CPU에 대입 -> 수학적 계산을 통해 예측
	4. 테스트 셋 삽입
	5. 예측(실제로 나온 데이터를 가지고 예측)

![[Pasted image 20250621212831.png]]

>[!question] 지금까지 어떻게 테스트셋을 만들지 않고 학습이 가능했을까? 정확도는 어떻게 측정함?
> => 데이터에 들어있는 모든 샘플을 그대로 테스트에 활용한 결과.
> 새로운 데이터에 적용했을때는 어느 정도의 성능이 나올지 알수 없다.

- 머신러닝의 최종 목적 : 과거의 데이터를 가지고 새로운 데이터를 예측 하는것.
	- 새로운 데이터에 사용할 모델을 만드는것.

>[!note] 
>층을 더하거나, 에포크 값을 높여 실행 횟수를 늘리면 정확도가 올라가나, 학습 데이터셋만으로 평가한 예측 성공률이 테스트셋에서도 그대로 나타나지는 않습니다.

 ![[Pasted image 20250621213535.png]]

- 학습을 진행해도 테스트 결과가 더 이상 좋아지지 않는 지점에서 학습을 멈춰야 한다.
	-  테스트 에러가 커지기 직전 w, b 값 구해야 함.

![[Pasted image 20250622102055.png]]

- 은닉층 개수가 올라감 -> 학습셋 예측률과 테스트셋의 예측률이 어떻게 변하는지 아래 표 참조
	- 24번째 갯수에서 테스트셋의 예측률이 떨어짐
- 이처럼 적절하게 조절하지 않을 경우 테스트 셋을 이용한 예측률은 오히려 떨어진다.

![[Pasted image 20250622102357.png]]

>[!note] 사이킷런 라이브러리
>파이썬으로 러닝을 실행할떄 필요한 전반적인 것들이 담긴 머신러닝의 필수 라이브러리

```python
from sklearn.model_selection import train_test_split

# 학습셋과 테스트셋 구분
X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.3, shuffle = True) 
```

- `train_test_split()` 함수 : 저장된 X, y 데이터에서 각각 정해진 비율만큼 학습셋과 테스트셋으로 분리시키는 함수
	- 여기서는 학습셋 70%, 테스트셋 30%로 지정
- `test_size` : 테스트셋의 비율
	- 0.3 : 전체 데이터의 30%를 테스트셋으로 설정
	- 나머지 70%는 학습셋으로 설정
- `shffle = True` : 나눠져 있는 데이터를 골고루 섞는다. 
- *X_train, X_test, y_train, y_test* -> 이거 순서 기억해라 중요함.

```python
score = model.evaluate(X_test, y_test)
print('Test accuracy : ', score[1])
```

- `model. evaluate()` : 만들어진 모델을 테스트셋에 적용할때 사용
	- `loss`, `accuracy` 두가지를 계산 해 출력한다. 이를 `score`에 저장 -> `accuracy` 출력
	- `score[1]` :  이므로, 평균을 계산해서 테스트 값의 accuracy를 보여준다.
	- `score[0]` 이라면, `loss` 값만 보여준다.

```python
from tesorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split

import pandas as pd

# 깃허브에 준비된 데이터를 가져옵니다.
!git clone https://github.com/taehojo/data.git

# 광물 데이터를 불러온다.
df = pd.read_csv('./data/sonar3.csv', header = None)

# 음파 관련 속성을 X로, 광물의 종류를 y 로 저장
X = df.iloc[:, 0:60]
y = df.iloc[:, 60]

# 학습셋과 테스트셋을 구분
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True)

# 모델을 설정
model = Sequential()
model.add(Dense(24, input_dim = 60, activation = 'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

# 모델을 컴파일 합니다.
model.compile(loss = 'binary_crossentropy' , optimizer = 'adam', metrics = ['accuracy'])

# 모델을 실행 합니다.
history = model.fit(X_train, y_train, epochs = 200, batch_size = 10)

# 모델을 테스트셋에 적용해 정확도를 구한다.
score = model.evaluate(X_test, y_test)
print('Test accuracy : ', score[1])
```

- 코드 분석
	- `compile`을 디폴트로 하면 `loss` 가 나오고, `accuracy`를 적어줘야 `loss`와 `accuracy`가  같이 나온다.
	- 모델을 실행 할때 : `X_train, y_train`
	- 모델을 테스트셋에 적용할때 : `X_test, y_test`



## 4. 모델 저장과 재사용

## 5. k겹 교차 검증