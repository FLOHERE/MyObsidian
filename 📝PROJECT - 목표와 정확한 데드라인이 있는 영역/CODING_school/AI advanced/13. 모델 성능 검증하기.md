- 예측 결과의 신뢰도를 높이는 방법을 알아봄
## 1. 데이터의 확인과 예측 실행
```python
df = pd.read_csv('./data.sonar3.csv', header = None)
# header : csv 파일의 제일 윗부분, 어떤 데이터인지 정보가 적혀있는 부분을 None으로 설정하면 없애고, True로 설정하면 보여줌
```

- 코드 실행 결과
	- 결과 데이터(소수점) : 0~1까지 평준화 (한번 가공한 데이터)
	- 맨 마지막 60번째 줄 : Label(실제 데이터)
		- 0 또는 1로 나온다. (binary classification)

![[Pasted image 20250621201038.png]]

- 실행 결과 분석
	- 전체가 61개의 열로 되어 있다.
	- 일반 암석 : 0
	- 광석 : 1
	- 1~60번째 열 : 음파 주파수의 에너지(0또는 1로 나타냄)

- 일반 암속과 광석이 각각 몇개 포함되는지 알아보는 코드
```python
df[60].value_counts() # 60번째 열의 데이터 수를 불러온다.
```

- 실행 결과
	- 1 : 광석의 개수 (111개)
	- 0 : 일반석의 개수 (97개)
	- `int 64` : 8바이트로 이루어진 데이터다.
	- 총 샘플 수 : 광석 개수 + 일반석 개수(208)
```txt
1    111 
0    97
Name : 60, dtype : int 64
```

```python
X = df.iloc[:, 0:60] # 0~59 번째 열 까지의 데이터 값을 가지고 오겠다는 뜻
y = df.iloc[:, 60] # 0 or 1 의 값으로 출력하겠다는 뜻
```

### ==시험문제 낼거다.== 초음파 광물 예측하기 : 데이터 확인과 실행

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 모델을 설정 합니다.
model = Sequential()
model.add(Dense(24, input_dim = 60, activation = 'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

# 모델 컴파일 합니다.
model.compile(loss = 'binary_crossentropy', optimizer = 'adam')
metrics = ['accuracy'] # loss + accuracy 값도 같이 나옴

# 모델을 실행 합니다.
history = model.fit(X, y, epochs = 200, batch_size = 10)
```

![[Pasted image 20250621204027.png]]

- 200번 반복되었을때 정확도가 100%가 나온다.
	- 이는 트레이닝 데이터(학습 데이터)를 그대로 테스트 데이터로 사용했기 때문에 올바른 정확도가 아니다. 
	- 테스트 데이터로 정확도를 다시 측정 해야 한다.
- **Binary Crossentropy**:

$$
\text{Loss} = - \left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]
$$
- y : 실제값 (0 또는 1)
- ŷ  : 예측값 (0 ~ 1 사이의 확률값)

>[!note] X 는 대문자고 y는 소문자인데 무슨 차이 인가여;;
> X 는 값이 두개 이상 나오는 "vector" 이고, y는 출력층 이므로 sigmoid 함수를 사용하기 때문에 값이 하나가 나오므로, 둘을 구분 하기 위해 각자 대문자, 소문자로 씁니다.

## 2. 과적합 이해하기
- 과적합(overfiting) : 학습 데이터에서는 일정 수준 이상의 예측 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는것
	- 학습셋 내부 성공률은 높아지나, 테스트셋에서 효과가 없다면 과적합이 일어나고 있는것.

![[Pasted image 20250621211734.png]]

- 과적합이 일어나는 이유
	1. 층이 너무 많을때
	2. 변수가 복잡할때
	3. 테스트 셋과 학습 셋이 중복 될때

## 3. 학습과 테스트 셋
- 과적합을 방지 하려면?
	- 학습셋과 테스트셋의 구분으로 해결
	- 학습과 동시에 테스트를 병행하며 진행하는 방법이 효율적
		- ex_) 총 데이터셋이 100개라면 70개는 학습셋으로, 나머지 30개는 테스트 셋으로 설정
- 모델 : 학습 결과를 저장한 파일
	- ex_) 신경망을 만들어 70개의 샘플로 학습 진행 -> 학습 결과 저장
	- 다른셋에 적용하여 그대로 다시 수행(재활용) 가능(다시 학습 시킬 필요 X)
	- 30개의 테스트셋으로 실험해 정확도를 살펴볼수 있다.(학습이 얼마나 잘되었는지 확인 가능)
- ==학습과 테스트 셋 순서==
	1. 학습셋 삽입
	2. 딥러닝 학습
	3. 모델에서 가중치와 바이어스 구함
		- `.hdf5` 파일 확장자로 저장 -> FPU, CPU에 대입 -> 수학적 계산을 통해 예측
	4. 테스트 셋 삽입
	5. 예측(실제로 나온 데이터를 가지고 예측)

![[Pasted image 20250621212831.png]]

>[!question] 지금까지 어떻게 테스트셋을 만들지 않고 학습이 가능했을까? 정확도는 어떻게 측정함?
> => 데이터에 들어있는 모든 샘플을 그대로 테스트에 활용한 결과.
> 새로운 데이터에 적용했을때는 어느 정도의 성능이 나올지 알수 없다.

- 머신러닝의 최종 목적 : 과거의 데이터를 가지고 새로운 데이터를 예측 하는것.
	- 새로운 데이터에 사용할 모델을 만드는것.

>[!note] 
>층을 더하거나, 에포크 값을 높여 실행 횟수를 늘리면 정확도가 올라가나, 학습 데이터셋만으로 평가한 예측 성공률이 테스트셋에서도 그대로 나타나지는 않습니다.

 ![[Pasted image 20250621213535.png]]



## 4. 모델 저장과 재사용

## 5. k겹 교차 검증